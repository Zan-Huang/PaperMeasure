{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PaperMeasure.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zan-Huang/PaperMeasure/blob/master/PaperMeasure.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "cLURdFJYMusQ",
        "colab_type": "code",
        "outputId": "d65a424d-4463-4386-906f-5c5c9a8323e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras import optimizers\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "import nltk\n",
        "import scipy\n",
        "\n",
        "from google.colab import drive\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from keras.models import load_model\n",
        "from keras.models import model_from_json\n",
        "\n",
        "from google.colab import auth\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "from googleapiclient.discovery import build\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2CiTLis-djDT",
        "colab_type": "code",
        "outputId": "b1a53a9b-3ff2-44b2-f644-7b1138c97dae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "K.tensorflow_backend._get_available_gpus()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/job:localhost/replica:0/task:0/device:GPU:0']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "RqlTiG-uPAjc",
        "colab_type": "code",
        "outputId": "c2d9c32f-7761-4212-81b5-861c70a8544d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')\n",
        "!ls\n",
        "\n",
        "!ls '/content/gdrive/My Drive/TMP'\n",
        "\n",
        "!cd '/content/gdrive/My Drive/TMPMEASURE'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "gdrive\tmodel.h5  model.json  modelscheck.hdf5\tsample_data\n",
            " 36.2.pdf   36.3.pdf  ' 36.4.pdf'   36.5.pdf   36.6.pdf   36.7.pdf   36.8.pdf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NnzgTKXlhbwv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm 'model.json'\n",
        "!rm 'model.h5'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KO3KD-VWPHyT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get install build-essential libpoppler-cpp-dev pkg-config python-dev"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1w1GYQ2ZTKy2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "yjWjJgCIbsD5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "text = \"The problem of an appropriate bibliography has proven to be of the hardest. To the best of the author's belief, there exists, aside from that here given, no bibliography aiming at a systematic classification of the sources and discussions of the mythology of the Latin-American Indians, as a whole. There[Pg viii] are, indeed, a considerable number of special bibliographies, regional in character, for which every student must be grateful; and it is hoped that not many of the more important of these have failed of inclusion in the bibliographical division devoted to; but for the whole field, the appended bibliography is pioneer work, and subject to the weaknesses of all such attempts. The principles of inclusion are: (1) All works upon which the text of the volume directly rests. These will be found cited in the Notes, where are also a few references to works cited for points of an adventitious character, and therefore not included in the general bibliography. (2) A more liberal inclusion of English and Spanish than of works in other languages, the one for accessibility, the other for source importance. (3) An effort to select only such works as have material directly pertinent to the mythology, not such as deal with the general culture, of the peoples under consideration,â€”a line most difficult to draw. In respect to bibliography, it should be further stated that it is the intent to enter the names of Spanish authors in the forms approved by the rules of the Real Academia, while it has not seemed important to follow other than the English custom in either text or notes. It is certainly the author's hope that the labour devoted to the assembling of the bibliography will prove helpful to students generally, and it is his belief that those wishing an introduction to the more important sources for the various regions will find of immediate help the select bibliographies given in the Notes, for each region and chapter.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yu5Iexo2Qzir",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "grammar_token_refined = []\n",
        "\n",
        "def nltk_word(string_text):\n",
        "  word_token = nltk.word_tokenize(string_text)\n",
        "  grammar_token = nltk.pos_tag(word_token)\n",
        "  for i in range(len(grammar_token)):\n",
        "    grammar_token_refined.append(grammar_token[i][1])\n",
        "  return word_token, grammar_token_refined"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "avgHERCDRf59",
        "colab_type": "code",
        "outputId": "70d16418-7473-4785-c6fc-19fd92e416af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "answer, grammar = nltk_word(text)\n",
        "word_dictionary = sorted(set(answer))\n",
        "grammar_dictionary = sorted(set(grammar))\n",
        "\n",
        "wordcount_vector = dict.fromkeys(word_dictionary, 0)\n",
        "grammarcount_vector = dict.fromkeys(grammar_dictionary, 0)\n",
        "\n",
        "def word_count(word_size, grammar_size):\n",
        "  for i in range(len(word_size)):\n",
        "    for j in range(len(word_dictionary)):\n",
        "      if (word_size[i] == word_dictionary[j]):\n",
        "        wordcount_vector[word_dictionary[j]] = wordcount_vector[word_dictionary[j]] + 1\n",
        "  for i in range(len(grammar_size)):\n",
        "    for j in range(len(grammar_dictionary)):\n",
        "      if (grammar[i] == grammar_dictionary[j]):\n",
        "        grammarcount_vector[grammar_dictionary[j]] = grammarcount_vector[grammar_dictionary[j]] + 1 \n",
        "  \n",
        "  return wordcount_vector, grammarcount_vector\n",
        "  \n",
        "final_input_word, final_input_grammar = word_count(answer, grammar)\n",
        "print(final_input_word)\n",
        "print(final_input_grammar)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'all': 1, 'assembling': 1, 'help': 1, 'enter': 1, 'indeed': 1, 'Indians': 1, 'wishing': 1, 'sources': 2, 'attempts': 1, 'grateful': 1, 'follow': 1, 'find': 1, 'seemed': 1, 'devoted': 2, 'regions': 1, 'certainly': 1, 'bibliography': 6, 'field': 1, '(': 3, 'appended': 1, 'character': 2, ',': 21, 'should': 1, 'forms': 1, 'failed': 1, 'only': 1, 'other': 3, 'text': 2, 'has': 2, 'hope': 1, 'division': 1, 'his': 1, 'to': 13, 'than': 2, 'rules': 1, 'Notes': 2, 'material': 1, 'every': 1, 'not': 4, 'aiming': 1, 'pertinent': 1, 'helpful': 1, 'notes': 1, 'with': 1, 'either': 1, 'each': 1, 'found': 1, 'works': 4, 'where': 1, 'exists': 1, 'principles': 1, 'culture': 1, 'Pg': 1, 'discussions': 1, 'further': 1, 'best': 1, 'subject': 1, 'for': 7, 'while': 1, 'pioneer': 1, '3': 1, 'various': 1, 'it': 5, ';': 2, 'be': 4, 'weaknesses': 1, 'belief': 2, 'importance': 1, 'here': 1, 'English': 2, 'consideration': 1, '[': 1, 'by': 1, 'chapter': 1, 'rests': 1, 'many': 1, 'region': 1, 'whole': 2, 'or': 1, 'classification': 1, 'inclusion': 3, 'There': 1, 'number': 1, 'one': 1, 'appropriate': 1, 'references': 1, 'names': 1, 'directly': 2, 'respect': 1, '.': 9, 'adventitious': 1, 'select': 2, 'languages': 1, 'given': 2, 'from': 1, 'deal': 1, 'prove': 1, 'introduction': 1, 'there': 1, 'custom': 1, 'The': 2, 'few': 1, '2': 1, 'therefore': 1, ':': 1, 'is': 5, 'more': 3, 'that': 5, 'These': 1, 'under': 1, 'but': 1, 'hoped': 1, 'volume': 1, 'Academia': 1, 'systematic': 1, 'authors': 1, 'line': 1, 'general': 2, 'effort': 1, 'considerable': 1, 'those': 1, 'must': 1, '\\xe2\\x80\\x94a': 1, 'these': 1, 'work': 1, 'will': 3, 'hardest': 1, 'viii': 1, 'of': 21, 'problem': 1, 'mythology': 2, 'are': 3, 'and': 7, 'Latin-American': 1, 'liberal': 1, 'stated': 1, 'cited': 2, 'accessibility': 1, 'proven': 1, 'an': 3, 'To': 1, 'as': 3, 'at': 1, 'have': 2, 'in': 8, 'Real': 1, 'bibliographical': 1, 'author': 2, ')': 3, 'generally': 1, 'aside': 1, '1': 1, 'also': 1, 'labour': 1, 'special': 1, 'which': 2, 'peoples': 1, 'difficult': 1, 'A': 1, 'draw': 1, 'regional': 1, \"'s\": 2, 'students': 1, 'In': 1, 'upon': 1, 'most': 1, 'important': 3, 'intent': 1, 'immediate': 1, 'student': 1, 'included': 1, 'such': 3, 'bibliographies': 2, ']': 1, 'approved': 1, 'source': 1, 'a': 4, 'All': 1, 'no': 1, 'It': 1, 'An': 1, 'points': 1, 'Spanish': 2, 'the': 34}\n",
            "{'PRP$': 1, 'VBG': 2, 'VBN': 15, 'POS': 2, 'VBP': 5, 'WDT': 2, 'JJ': 37, 'VBZ': 9, 'DT': 53, 'NN': 48, ')': 3, '(': 3, ',': 21, '.': 9, 'TO': 14, 'PRP': 6, 'RB': 15, ':': 3, 'NNS': 25, 'NNP': 5, 'VB': 10, 'WRB': 1, 'CC': 10, 'RBS': 1, 'RBR': 3, 'CD': 4, 'EX': 2, 'IN': 54, 'MD': 5, 'NNPS': 1, 'JJS': 2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bDDB5HDVcFb0",
        "colab_type": "code",
        "outputId": "3aa969d5-cce4-446b-c2f2-c799de86a09d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "cell_type": "code",
      "source": [
        "train_list = []\n",
        "grammar_list = []\n",
        "for key, value in wordcount_vector.iteritems():\n",
        "  train_list.append(value)\n",
        "  \n",
        "for key, value in grammarcount_vector.iteritems():\n",
        "  grammar_list.append(value)\n",
        "\n",
        "  \n",
        "X_raw = np.array(train_list)\n",
        "X_normalized = np.log(X_raw)\n",
        "\n",
        "grammar_raw = np.array(grammar_list)\n",
        "grammar_normalized = np.log(grammar_raw)\n",
        "\n",
        "print(X_normalized)\n",
        "print(grammar_normalized)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.         0.         0.         0.         0.         0.\n",
            " 0.         0.69314718 0.         0.         0.         0.\n",
            " 0.         0.69314718 0.         0.         1.79175947 0.\n",
            " 1.09861229 0.         0.69314718 3.04452244 0.         0.\n",
            " 0.         0.         1.09861229 0.69314718 0.69314718 0.\n",
            " 0.         0.         2.56494936 0.69314718 0.         0.69314718\n",
            " 0.         0.         1.38629436 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         1.38629436\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         1.94591015 0.         0.\n",
            " 0.         0.         1.60943791 0.69314718 1.38629436 0.\n",
            " 0.69314718 0.         0.         0.69314718 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.69314718\n",
            " 0.         0.         1.09861229 0.         0.         0.\n",
            " 0.         0.         0.         0.69314718 0.         2.19722458\n",
            " 0.         0.69314718 0.         0.69314718 0.         0.\n",
            " 0.         0.         0.         0.         0.69314718 0.\n",
            " 0.         0.         0.         1.60943791 1.09861229 1.60943791\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.69314718 0.         0.\n",
            " 0.         0.         0.         0.         0.         1.09861229\n",
            " 0.         0.         3.04452244 0.         0.69314718 1.09861229\n",
            " 1.94591015 0.         0.         0.         0.69314718 0.\n",
            " 0.         1.09861229 0.         1.09861229 0.         0.69314718\n",
            " 2.07944154 0.         0.         0.69314718 1.09861229 0.\n",
            " 0.         0.         0.         0.         0.         0.69314718\n",
            " 0.         0.         0.         0.         0.         0.69314718\n",
            " 0.         0.         0.         0.         1.09861229 0.\n",
            " 0.         0.         0.         1.09861229 0.69314718 0.\n",
            " 0.         0.         1.38629436 0.         0.         0.\n",
            " 0.         0.         0.69314718 3.52636052]\n",
            "[0.         0.69314718 2.7080502  0.69314718 1.60943791 0.69314718\n",
            " 3.61091791 2.19722458 3.97029191 3.87120101 1.09861229 1.09861229\n",
            " 3.04452244 2.19722458 2.63905733 1.79175947 2.7080502  1.09861229\n",
            " 3.21887582 1.60943791 2.30258509 0.         2.30258509 0.\n",
            " 1.09861229 1.38629436 0.69314718 3.98898405 1.60943791 0.\n",
            " 0.69314718]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "edB5lc4rq8cQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here, we parsed whatever text file was fed by the PDF and then created a bag of words, unnormalized however. The bag of words will be normalized later after a numpy array conversion by the size of the corpus."
      ]
    },
    {
      "metadata": {
        "id": "2e38h2-NqrIb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**THIS IS ALL THE DATA WE NEED, ZAN JUST PUT THE FILE I SENT YOU IN THE COLAB.**"
      ]
    },
    {
      "metadata": {
        "id": "3yfuK-V-q7dH",
        "colab_type": "code",
        "outputId": "f74e1ba3-8382-44d1-81fa-996eb018cdde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "dataIn = np.load(\"/content/gdrive/My Drive/TMPMEASURE/preppedData.npy\")\n",
        "dataOut = np.load(\"/content/gdrive/My Drive/TMPMEASURE/preppedDataKey.npy\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(dataIn, dataOut, test_size=0.10)\n",
        "\n",
        "size_of_input = dataIn.shape[1]\n",
        "print x_train.size\n",
        "print y_train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18244980\n",
            "[[0 1]\n",
            " [0 1]\n",
            " [1 0]\n",
            " ...\n",
            " [1 0]\n",
            " [0 1]\n",
            " [1 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ORM1bhsuZ69l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm 'modelscheck.hdf5'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5WYFrufBTGID",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(units = 4000,  kernel_initializer='glorot_normal', bias_initializer='zeros', activation='relu', input_dim=size_of_input))\n",
        "model.add(Dense(units = 1000, kernel_initializer='glorot_normal', bias_initializer='zeros', activation='relu'))\n",
        "model.add(Dense(units = 50, kernel_initializer='glorot_normal', bias_initializer='zeros', activation='relu'))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
        "\n",
        "model_json = model.to_json()\n",
        "\n",
        "checkpoint = [ModelCheckpoint(filepath='modelscheck.hdf5')]\n",
        "\n",
        "\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# make sure you save the model zan!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GkPdX_A4VIP_",
        "colab_type": "code",
        "outputId": "f0fe3f61-fd24-44be-b001-d947d4fb9539",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=adam,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          validation_data = (x_test, y_test),\n",
        "          epochs=8,\n",
        "          batch_size=1,\n",
        "          callbacks = checkpoint,\n",
        "          verbose=1)\n",
        "\n",
        "model.save('PaperMeasure/model.h5')"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "78/78 [==============================] - 0s 4ms/step\n",
            "Train on 702 samples, validate on 78 samples\n",
            "Epoch 1/8\n",
            "702/702 [==============================] - 92s 131ms/step - loss: 0.4486 - acc: 0.7991 - val_loss: 0.2984 - val_acc: 0.8846\n",
            "Epoch 2/8\n",
            "702/702 [==============================] - 89s 127ms/step - loss: 0.0559 - acc: 0.9886 - val_loss: 0.3738 - val_acc: 0.9231\n",
            "Epoch 3/8\n",
            "702/702 [==============================] - 90s 128ms/step - loss: 0.0738 - acc: 0.9872 - val_loss: 1.2913 - val_acc: 0.8462\n",
            "Epoch 4/8\n",
            "702/702 [==============================] - 89s 127ms/step - loss: 0.0589 - acc: 0.9886 - val_loss: 0.9852 - val_acc: 0.8974\n",
            "Epoch 5/8\n",
            "702/702 [==============================] - 89s 127ms/step - loss: 0.0716 - acc: 0.9886 - val_loss: 1.0931 - val_acc: 0.8846\n",
            "Epoch 6/8\n",
            "702/702 [==============================] - 89s 127ms/step - loss: 0.0716 - acc: 0.9829 - val_loss: 0.7333 - val_acc: 0.8846\n",
            "Epoch 7/8\n",
            "702/702 [==============================] - 89s 127ms/step - loss: 2.7409e-04 - acc: 1.0000 - val_loss: 1.4458 - val_acc: 0.8590\n",
            "Epoch 8/8\n",
            "702/702 [==============================] - 89s 127ms/step - loss: 1.9741e-07 - acc: 1.0000 - val_loss: 1.4648 - val_acc: 0.8718\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3xFpth8_Yh2j",
        "colab_type": "code",
        "outputId": "ea1c3cba-8e08-4cc4-8e5f-44f1e9ab4388",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print(scores[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "78/78 [==============================] - 0s 833us/step\n",
            "0.8461538430971977\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Y3DWRR_8aOfg",
        "colab_type": "code",
        "outputId": "2a451843-2c52-40d0-c65a-80730b27c327",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "%ls"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LICENSE  model.h5  PaperMeasure  README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eOSj0pnRnFfm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!git add model.h5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FUYyvS1WGq7y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0a9dd679-ba11-4747-f01f-09ebde49a001"
      },
      "cell_type": "code",
      "source": [
        "!git commit -m \"FINALLY\""
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[master d137a01] FINALLY\n",
            " 1 file changed, 0 insertions(+), 0 deletions(-)\n",
            " create mode 100644 model.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "O6x4r-DUQqDo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!sshpass -p \"zanhuang\" scp 'model.md5' alex@dp.ton.io:/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AWBWfQcuPX_l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}